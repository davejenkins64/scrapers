# Scrapers

This project will be a web page scraper for getting my local daily
tv listings and putting them into a database.

## Plan

The plan is to use various languages, frameworks and backends to do the same
task in different ways to compare and contrast the technologies.

Use git to store progress and push any milestones on each to github.

Use docker as the development and runtime environment for each approach.
Perhaps swarm or k8s if a single container won't do.

Use TDD, red/green/purple to write the tests first, make them pass, then 
refactor.

By backends I mean databases, think flat files, sqlite, mysql and postgress.

## Perl

### Modules

#### Web Scraping

Found this page to use as a guide: 
https://www.scrapingbee.com/blog/web-scraping-perl/

#### TDD

Next I'll need a little tutorial for TDD in perl.

Plan to use vim as my IDE for now, but perhaps switch to Visual Studio Code
later or for the other languages?

### Perl Development in Docker

I'll need a development container and a strategy to build a production 
container.
Start here: https://hub.docker.com/_/perl

### sqlite
### mysql
### postgress

## Python

## Typescript

## Go

## Rust

## C++

## git and github plans

Plan to create a scratch feature branch for daily commits and cherry pick to 
a feature branch to make the merge history coherent.  
The feature branch then becomes a pull request to be merged into master.

